{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run Parameters\n",
    "\n",
    "use_dataset: which dataset to run examples for {either H36m or 3DHP}\n",
    "example: which example to use (int [0,5)\n",
    "\"\"\"\n",
    "\n",
    "use_dataset ='H36m'\n",
    "# use_dataset ='3DHP'\n",
    "\n",
    "\r\n",
    "\r\n",
    "example = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings('ignore')\r\n",
    "import sys\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import torchvision\r\n",
    "import pickle\r\n",
    "import os\r\n",
    "\r\n",
    "from model import LinearModel, weight_init\r\n",
    "\r\n",
    "import dataset_h36m\r\n",
    "from dataset_h36m import H36MDataset\r\n",
    "import dataset_3dhp\r\n",
    "from dataset_3dhp import MpiInf3dDataset\r\n",
    "\r\n",
    "import utils\r\n",
    "from utils import batch_normalize_canon_pcl_human_joints, batch_normalize_canon_human_joints, denorm_human_joints\r\n",
    "\r\n",
    "import pcl\r\n",
    "import pcl_util\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import plotly.graph_objects as go\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from pylab import *\r\n",
    "from PIL import Image\r\n",
    "import constants\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config_data import get_dataset_config\n",
    "dataset_config = get_dataset_config(use_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plotting Functions used in this script\"\"\"\n",
    "\n",
    "def plotPerspectiveCrop(ax_img, i, positions, scales, width, height, Ks, position_py, scale_py, lwidth=5, c_persp=\"blue\", c_affine = \"red\"):\n",
    "    P_virt2orig, R_virt2orig, K_virt = pcl.pcl_transforms(positions, scales, Ks, focal_at_image_plane=True, slant_compensation=True)\n",
    "    grid_sparse = pcl.perspective_grid(P_virt2orig, torch.tensor([256, 256]), torch.Size([3, 3]), transform_to_pytorch=True)\n",
    "\n",
    "    # convert back from pytorch coordinates\n",
    "    xv = (grid_sparse[i,:,:,0].numpy()+1)/2 * width\n",
    "    yv = (grid_sparse[i,:,:,1].numpy()+1)/2 * height\n",
    "\n",
    "    # plot grid (once horizontal, once vertical lines)\n",
    "    ax_img.plot(xv,yv,'-',linewidth=lwidth,color=c_persp)\n",
    "    ax_img.plot(xv.transpose(),yv.transpose(),'-',linewidth=lwidth,color=c_persp)\n",
    "\n",
    "    # plot rectangular crop\n",
    "    rect = matplotlib.patches.Rectangle(\n",
    "        (width  * ((position_py[i,0]+1)/2-scale_py[i,0]/2), height * ((position_py[i,1]+1)/2-scale_py[i,1]/2)),\n",
    "        scale_py[i,0] * width, scale_py[i,1] * height,\n",
    "        linewidth=lwidth, linestyle='dashed', edgecolor=c_affine, facecolor='none')\n",
    "    ax_img.add_patch(rect)\n",
    "    \n",
    "bones_h36m = [[0,1], [1, 15], [2,1], [3, 2], [4,3], [5,1], [6,5], [7,6], [8,14], [9,8], [10,9], [11, 14], [12, 11], [13,12], [14,14], [15,14], [16,1]]\n",
    "def plot_3Dpose(ax, pose_3d_1, bones=bones_h36m, linewidth=5, alpha=0.95, colormap='gist_rainbow', autoAxisRange=True, flip_yz=True, change_view=True, isGT=False):\n",
    "    cmap = plt.get_cmap(colormap)\n",
    "    pose_3d = pose_3d_1.clone()\n",
    "    pose_3d = np.reshape(pose_3d.numpy().transpose(), (3, -1))\n",
    "    pose_3d[1,:] *= -1\n",
    "\n",
    "    if flip_yz:\n",
    "        X, Y, Z = np.squeeze(np.array(pose_3d[0, :])), np.squeeze(np.array(pose_3d[2, :])), np.squeeze(\n",
    "            np.array(pose_3d[1, :]))\n",
    "    else:\n",
    "        X, Y, Z = np.squeeze(np.array(pose_3d[0, :])), np.squeeze(np.array(pose_3d[1, :])), np.squeeze(\n",
    "            np.array(pose_3d[2, :]))\n",
    "    XYZ = np.vstack([X, Y, Z])\n",
    "\n",
    "    if change_view:\n",
    "        ax.view_init(elev=0, azim=-90)\n",
    "    cmap = plt.get_cmap(colormap)\n",
    "\n",
    "    maximum = len(bones)\n",
    "\n",
    "    if not isGT:\n",
    "        for i, bone in enumerate(bones):\n",
    "            colorIndex = cmap.N - cmap.N * i/float(maximum) # cmap.N - to start from back (nicer color)\n",
    "            color = cmap(int(colorIndex))\n",
    "            depth = max(XYZ[1, bone])\n",
    "            zorder = -depth # otherwise bones with be ordered in the order of drawing or something even more weird...\n",
    "            ax.plot(XYZ[0, bone], XYZ[1, bone], XYZ[2, bone], color=color, linewidth=linewidth, zorder=zorder,\n",
    "                                alpha=alpha, solid_capstyle='round')\n",
    "    else:\n",
    "        for i, bone in enumerate(bones):\n",
    "            depth = max(XYZ[1, bone])\n",
    "            zorder = -depth # otherwise bones with be ordered in the order of drawing or something even more weird...\n",
    "            ax.plot(XYZ[0, bone], XYZ[1, bone], XYZ[2, bone], color='black', linewidth=linewidth, zorder=zorder,\n",
    "                                alpha=alpha, solid_capstyle='round')\n",
    "\n",
    "    # maintain aspect ratio\n",
    "    if autoAxisRange:\n",
    "        max_range = np.array([X.max() - X.min(), Y.max() - Y.min(), Z.max() - Z.min()]).max() / 2.0\n",
    "\n",
    "        mid_x = (X.max() + X.min()) * 0.5\n",
    "        mid_y = (Y.max() + Y.min()) * 0.5\n",
    "        mid_z = (Z.max() + Z.min()) * 0.5\n",
    "        ax.set_xlim(mid_x - max_range, mid_x + max_range)\n",
    "        ax.set_ylim(mid_y - max_range, mid_y + max_range)\n",
    "        ax.set_zlim(mid_z - max_range, mid_z + max_range)\n",
    "\n",
    "def plot_qualitative_pose(ax, plot_image, canon_label_no_norm, factor=8, x_off=128, y_off=-128, z_off=-10000, colormap='gist_rainbow', linewidth=5, isGT=False, elev=10, azim=-90):\n",
    "    if plot_image is not None:\n",
    "        plot_3Dpose(ax, (canon_label_no_norm/factor)+torch.tensor([x_off, y_off, z_off]), flip_yz=True, autoAxisRange=False, colormap=colormap, linewidth=linewidth)\n",
    "    else:\n",
    "        if not isGT:\n",
    "            plot_3Dpose(ax, canon_label_no_norm, flip_yz=True, autoAxisRange=True, colormap=colormap, linewidth=linewidth)\n",
    "        else:\n",
    "            plot_3Dpose(ax, canon_label_no_norm, flip_yz=True, autoAxisRange=False, colormap=colormap, isGT=isGT, linewidth=linewidth)\n",
    "    if plot_image is not None:\n",
    "        ax.plot_surface(x,y,z, rstride=1, cstride=1, facecolors=plot_image)\n",
    "        ax.set_xlim(0, img.shape[0])\n",
    "        ax.set_zlim(0,img.shape[1])\n",
    "        ax.view_init(elev=10, azim=-90)\n",
    "\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(),visible=False)\n",
    "    plt.setp(ax.get_yticklabels(),visible=False)\n",
    "    plt.setp(ax.get_zticklabels(),visible=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code for preping input 2D poses to be handled by the network\n",
    "PCL Preprocess: Converts the 2D poses to virtual camera coordinates and then normalizes them\n",
    "PCL Postprocess: Converts the output (from the network) 3D poses from virtual camera to camera coordinates\n",
    "\"\"\"\n",
    "\n",
    "def pcl_preprocess(batch_size, num_joints, canon_label_2d_with_hip, orig_img_shape, Ks_px_orig, location, scale, \\\n",
    "    normalize=True, use_slant_compensation=False, use_dataset='H36m'):\n",
    "\n",
    "    canon_virt_2d, R_virt2orig, P_virt2orig = pcl.pcl_transforms_2d(canon_label_2d_with_hip, location, scale, Ks_px_orig,\\\n",
    "                                                                    focal_at_image_plane=True, \\\n",
    "                                                                    slant_compensation=use_slant_compensation)\n",
    "    model_input = canon_virt_2d.clone()\n",
    "    \n",
    "    if normalize:\n",
    "        mean_2d, std_2d = dataset_config.get_2d_mean_std(slant=use_slant_compensation, stn=False)\n",
    "        model_input = utils.batch_normalize_canon_pcl_human_joints(model_input, mean_2d, std_2d)\n",
    "\n",
    "    model_input = model_input.view(batch_size, -1)\n",
    "\n",
    "    return {'model_input':model_input, 'canon_virt_2d':canon_virt_2d, 'R_virt2orig':R_virt2orig, 'P_virt2orig':P_virt2orig}\n",
    "\n",
    "def pcl_postprocess(batch_size, num_joints, output, R_virt2orig, device='cpu', use_dataset='H36m'):\n",
    "\n",
    "    dataset_config = get_dataset_config(use_dataset)\n",
    "    canonical_mean = dataset_config.get_joint_mean()\n",
    "    canonical_std = dataset_config.get_joint_std()\n",
    "    \n",
    "    R_virt2orig = R_virt2orig\n",
    "    new_pre_transform = output.view(batch_size, num_joints, -1)\n",
    "    new_pre_transform = denorm_human_joints(new_pre_transform, canonical_mean, canonical_std).unsqueeze(3)\n",
    "\n",
    "    new_output = pcl.virtPose2CameraPose(new_pre_transform, R_virt2orig, batch_size, num_joints)\n",
    "\n",
    "    normalized_output = batch_normalize_canon_human_joints(new_output, mean=canonical_mean, std=canonical_std)\n",
    "    new_pre_transform = new_pre_transform.squeeze(-1).view(batch_size, num_joints, -1)\n",
    "\n",
    "    new_pre_transform = batch_normalize_canon_human_joints(new_pre_transform, mean=canonical_mean, std=canonical_std)\n",
    "    return {\"pre_transform\":new_pre_transform, \"output\":normalized_output, 'output_no_norm':new_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load and preprocess 2D pose and pass those inputs through two pretrained networks (one trained with PCL and \n",
    "one trained with rectangular crops). This script shows how to use the \"PCL Preprocess\" (converting from Camera 2D pose\n",
    "to virtual camera 2D pose) and the \"PCL Postprocess\" (which converts predicted a 3D pose in virtual camera coordinates \n",
    "back to camera coordinates)\n",
    "\"\"\"\n",
    "\n",
    "pcl_output = None\n",
    "stn_output = None\n",
    "\n",
    "if use_dataset == 'H36m':\n",
    "    with open('imgs/H36m-sample.pickle', 'rb') as f:\n",
    "        test = pickle.load(f)\n",
    "    data = test[example]\n",
    "else:\n",
    "    with open('imgs/3DHP-sample.pickle', 'rb') as f:\n",
    "        test = pickle.load(f)\n",
    "    data = test[example]\n",
    "\n",
    "for i in range(2):\n",
    "    \"\"\"\n",
    "    SELECT WHICH MODEL WILL BE LOADED\n",
    "        - Note the model selected for H36m and 3DHP where trained with different parameters\n",
    "          the H36m models were trained with knowledge of the 3D root while the 3DHP models\n",
    "          were trained when only knowing the 2D scale.\n",
    "    \"\"\"\n",
    "    use_pcl = i\n",
    "    if use_dataset == 'H36m':\n",
    "        model_dir = f'demo_models/2D-3D/H36m/{\"PCL-2DGT-3DRootGT\" if use_pcl else \"STN-2DGT-3DRootGT\"}'\n",
    "    else:\n",
    "        model_dir = f'demo_models/2D-3D/3DHP/{\"PCL-2DGT-2DScale\" if use_pcl else \"STN-2DGT-2DScale\"}'\n",
    "\n",
    "    \"\"\"LOAD IN THE MODEL\"\"\"\n",
    "    model = LinearModel()\n",
    "    model_file = os.path.join(model_dir, \"lowest_validation_model.tar\")\n",
    "    checkpoint = torch.load(model_file, map_location='cuda:0')\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "\n",
    "    \"\"\"READ IN DATA\"\"\"\n",
    "    # Image Inputs\n",
    "    img = data['img']\n",
    "    \n",
    "    # PCL Parameters\n",
    "    Ks_px_orig = data['Ks_px']\n",
    "    orig_img_shape = data['orig_img_shape']\n",
    "    location_px = data['crop_location_px'].float()\n",
    "    scale_px = data['crop_scale_px'].float()\n",
    "    \n",
    "    # Pose 3D\n",
    "    label_3d = data['label_3d']\n",
    "    \n",
    "    # Pose 2D\n",
    "    label_2d_px = data['label_2d_px'] # this includes the hip location and is NOT canonical pose (for H36m)\n",
    "\n",
    "    \"\"\"DATA PREPROCESSING FOR STN INPUT\"\"\"\n",
    "    square_scale = torch.tensor([torch.max(scale_px.squeeze(0)), torch.max(scale_px.squeeze(0))])\n",
    "    square_scale_py = square_scale / orig_img_shape.squeeze(0)\n",
    "\n",
    "    scale_py = square_scale_py.unsqueeze(0)\n",
    "    location_py = utils.pixel_2_pytorch_locations(location_px, orig_img_shape[:,0], orig_img_shape[:,1])\n",
    "\n",
    "    if use_dataset == \"H36m\":\n",
    "        hips = label_2d_px[:,0,:].unsqueeze(1).repeat(1,label_2d_px.shape[1],1)\n",
    "    else:\n",
    "        hips = hips = label_2d_px[:,14,:].unsqueeze(1).repeat(1,label_2d_px.shape[1],1)\n",
    "\n",
    "    label_2d_no_hip = label_2d_px - hips\n",
    "    \n",
    "    \n",
    "    \"\"\"ENSURE THAT ALL INPUTS AND LABELS ARE IN CANONICAL FORM\"\"\"\n",
    "    \n",
    "    label_2d_px = dataset_config.to_canonical(label_2d_px)\n",
    "    canon_label_2d = dataset_config.to_canonical(label_2d_no_hip)\n",
    "    label_3d = dataset_config.to_canonical(label_3d)\n",
    "\n",
    "    \n",
    "    \n",
    "    num_joints, bs = label_2d_px.shape[1], label_2d_px.shape[0] # computes the number of joints in the pose  # computes the batch size of the input\n",
    "\n",
    "    \"\"\"PROCESSING THE INPUTS FOR PCL\"\"\"\n",
    "    if use_pcl:\n",
    "        preprocess = pcl_preprocess(bs, num_joints, label_2d_px, orig_img_shape, Ks_px_orig, location_px, scale_px, \n",
    "                                    normalize=True, use_slant_compensation=(use_dataset != 'H36m'), use_dataset=use_dataset)\n",
    "        model_input = preprocess['model_input']\n",
    "        canon_virt_2d = preprocess['canon_virt_2d']\n",
    "        R_virt2orig = preprocess['R_virt2orig']        \n",
    "    else:\n",
    "        model_input = canon_label_2d.detach().clone()\n",
    "        model_input = model_input / scale_py.unsqueeze(1)\n",
    "        mean, std = dataset_config.get_2d_mean_std(slant=False, stn=False, use_pcl=False)\n",
    "        model_input = batch_normalize_canon_human_joints(model_input, mean, std)\n",
    "        model_input = model_input.view(bs, -1)\n",
    "        \n",
    "\n",
    "    \"\"\"PASS THE INPUT 2D POSE TO THE MODEL WHICH PREDICTS A 3D POSE\"\"\"\n",
    "    output = model(model_input)\n",
    "\n",
    "    \"\"\"USING THE PCL POSTPROCESS TO CONVERT PREDICTED 3D POSE FROM VIRTUAL CAMERA COORDINATES TO CAMERA COORDINATES\"\"\"\n",
    "    if use_pcl:\n",
    "        postprocess = pcl_postprocess(bs, num_joints, output, R_virt2orig, 'cpu', use_dataset=use_dataset)\n",
    "        output = postprocess['output_no_norm']\n",
    "        normalized_output = postprocess['output']\n",
    "        pre_transform = postprocess['pre_transform']\n",
    "    else: \n",
    "        output = output.view(bs, -1, 3)\n",
    "        canonical_mean = dataset_config.get_joint_mean()\n",
    "        canonical_std = dataset_config.get_joint_std()\n",
    "        output = denorm_human_joints(output, canonical_mean, canonical_std)\n",
    "\n",
    "    \"\"\"STORE PREDICTED POSES FOR PLOTTING LATER ON\"\"\"\n",
    "    if use_pcl:\n",
    "        pcl_output = output.detach().clone()\n",
    "    else:\n",
    "        stn_output = output.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PCL AND RECTANGULAR CROP VISUALIZATION\n",
    "Similar to the pcl_demo.ipynb script, we visualize the PCL and rectangualr crop on the original image\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111)\n",
    "input_img = Image.fromarray((img.squeeze(0).permute(1,2,0).numpy()*255).astype(uint8))\n",
    "ax.imshow(input_img)\n",
    "img_w_h_shape = torch.tensor([256, 256])\n",
    "use_location = location_px / orig_img_shape * img_w_h_shape.unsqueeze(0)\n",
    "use_scale = scale_px / orig_img_shape * img_w_h_shape\n",
    "Ks = pcl_util.K_new_resolution_px(Ks_px_orig, orig_img_shape, img_w_h_shape)\n",
    "plotPerspectiveCrop(ax, 0, use_location, use_scale, 256, 256, Ks, location_py, scale_py, lwidth=4)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "VISUALIZING THE 3D POSES AGAINST THE RESPECTIVE CROPS\n",
    "Here we can see that the predicted virtual camera pose lines up better with the PCL crop image than the pose predicted\n",
    "with root centering and the rectangular crop.\n",
    "\"\"\"\n",
    "\n",
    "P_virt2orig, R_virt2orig, K_virt = pcl.pcl_transforms(use_location, use_scale, Ks, focal_at_image_plane=True, slant_compensation=True)\n",
    "grid_perspective = pcl.perspective_grid(P_virt2orig, img_w_h_shape,\\\n",
    "        (img_w_h_shape), transform_to_pytorch=True)\n",
    "grid_sparse = pcl.perspective_grid(P_virt2orig, img_w_h_shape,\\\n",
    "        torch.tensor([3, 3]), transform_to_pytorch=True)\n",
    "\n",
    "affine_matrix = torch.zeros([1,2,3])\n",
    "affine_matrix[:,0,0] = square_scale_py[0]\n",
    "affine_matrix[:,1,1] = square_scale_py[1]\n",
    "affine_matrix[:,:2,2] = location_py\n",
    "\n",
    "affine_grid = F.affine_grid(affine_matrix, img.size(), align_corners=False)\n",
    "\n",
    "pcl_crop = F.grid_sample(img.clone(), grid_perspective, align_corners=False)\n",
    "stn_crop = F.grid_sample(img.clone(), affine_grid, align_corners=False)\n",
    "\n",
    "canonical_mean = dataset_config.get_joint_mean()\n",
    "canonical_std = dataset_config.get_joint_std()\n",
    "    \n",
    "denorm_pre_transform = denorm_human_joints(pre_transform, canonical_mean, canonical_std).detach()\n",
    "\n",
    "img = pcl_crop.detach().cpu().squeeze(0).permute(1,2,0)\n",
    "z,x= ogrid[0:img.shape[0], 0:img.shape[1]]\n",
    "y = np.atleast_2d(50)\n",
    "im = Image.fromarray((img.numpy()*255).astype(np.uint8))\n",
    "image = im.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "image.save('imgs/dummy.png', format='png')\n",
    "plot_image = plt.imread('imgs/dummy.png')\n",
    "\n",
    "img = stn_crop.detach().cpu().squeeze(0).permute(1,2,0)\n",
    "z,x= ogrid[0:img.shape[0], 0:img.shape[1]]\n",
    "y = np.atleast_2d(50)\n",
    "im = Image.fromarray((img.numpy()*255).astype(np.uint8))\n",
    "image = im.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "image.save('imgs/dummy.png', format='png')\n",
    "plot_image_stn = plt.imread('imgs/dummy.png')\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "plot_qualitative_pose(ax, plot_image, denorm_pre_transform, factor=8, x_off=128, y_off=-128, z_off=-10000, colormap='winter')\n",
    "ax.set_title(\"PCL Crop and Virtual Camera Pose\")\n",
    "plt.margins(0,0,0)\n",
    "ax1 = fig.add_subplot(122, projection='3d')\n",
    "plot_qualitative_pose(ax1, plot_image_stn, stn_output.clone(), factor=8, x_off=128, y_off=-128, z_off=-10000, colormap='autumn')\n",
    "ax1.set_title(\"STN Crop and GT Pose\")\n",
    "plt.margins(0,0,0)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "VISUALIZING PREDICTED PCL POSE AND ROOT CENTERED POSE AGAINST GT LABEL\n",
    "Here we can see that the PCL predicted pose is better aligned with the GT label than the root-centered predicted pose\n",
    "# \"\"\"\n",
    "gt = label_3d\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "plot_qualitative_pose(ax, None, gt.detach(), factor=8, x_off=128, y_off=-128, z_off=-10000, colormap='Greys', isGT=True, linewidth=5)\n",
    "plot_qualitative_pose(ax, None, pcl_output.detach(), factor=8, x_off=128, y_off=-128, z_off=-10000, colormap='winter',linewidth=5)\n",
    "plt.axis(\"Off\")\n",
    "ax.set_title(\"PCL Pose (Blue) and GT (Black)\")\n",
    "\n",
    "ax1 = fig.add_subplot(122, projection='3d')\n",
    "plot_qualitative_pose(ax1, None, gt.detach(), factor=8, x_off=128, y_off=-128, z_off=-10000, colormap='Greys', isGT=True, linewidth=5)\n",
    "plot_qualitative_pose(ax1, None, stn_output.detach(), factor=8, x_off=128, y_off=-128, z_off=-10000, colormap='autumn',linewidth=5)\n",
    "ax1.set_title(\"Root Centering Pose (Orange) and GT (Black)\")\n",
    "fig.tight_layout()\n",
    "plt.margins(0,0,0)\n",
    "plt.axis(\"Off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pcl)",
   "language": "python",
   "name": "pcl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
